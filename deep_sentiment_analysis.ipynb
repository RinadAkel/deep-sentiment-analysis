{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RinadAkel/deep-sentiment-analysis/blob/main/deep_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfxfPWS-tk-3"
      },
      "source": [
        "© 2021 Zaka AI, Inc. All Rights Reserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT3_7pSqQtmQ"
      },
      "source": [
        "#Deep Sentiment Analysis\n",
        "\n",
        "**Objective:** The goal from this exercise is to learn how to integrate Deep Learning into Natural Language Processing through Deep Sentiment Analysis.\n",
        "The sections of this colab exercise are:\n",
        "1. Keras Embedding Layer\n",
        "2. Dataset loading\n",
        "3. Data preparation\n",
        "4. Feature extraction using Word Embeddings\n",
        "5. Recurrent Neural Network model\n",
        "6. Plotting training details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLzGR6lhUF9g"
      },
      "source": [
        "# Keras Embedding Layer\n",
        "\n",
        "Before we start with the Sentiment Analysis exercise, let's look at an example of how to use a Keras Embedding layer. \n",
        "\n",
        "In this example, we will build a `Sequential` model with an `Embedding` layer to learn the embeddings of a series of simple documents defined in the docs variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRpZ3TlXUBsZ"
      },
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# define documents\n",
        "docs = ['Well done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Weak',\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.']\n",
        "\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
        "\n",
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "print(encoded_docs)\n",
        "\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n",
        "\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# summarize the model\n",
        "model.summary()\n",
        "\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Weql_nx1bKZI"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "Text classification is one of the important tasks of text mining.\n",
        "\n",
        "![alt text](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535125878/NLTK3_zwbdgg.png)\n",
        "\n",
        "In this notebook, we will perform Sentiment Analysis on IMDB movies reviews. Sentiment Analysis is the art of extracting people's opinion from digital text. We will use a regression model from Scikit-Learn able to predict the sentiment given a movie review. \n",
        "\n",
        "We will use [the IMDB movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which consists of 50,000 movies review (50% are positive, 50% are negative).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is the same exercise we saw in the previous lesson but the differences here are:\n",
        "\n",
        "\n",
        "*   We are using **Word Embeddings** for feature extraction instead of Bag-of-Words. This is done by adding an `Embedding` layer as the first layer in the Sequential model.\n",
        "*   We are using a deep **Recurrent Neural Network** for modeling.\n",
        "\n",
        "These changes should allow the model to better understand the dataset and give better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NG9CLMaT0N7"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_HVs_6nS2F2"
      },
      "source": [
        "### 1. Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxDGpou5cAzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dbb0f7c-77ff-4c4b-91fc-c54890ed0407"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "# download Punkt Sentence Tokenizer\n",
        "nltk.download('punkt')\n",
        "# download stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5blEsxShTEV"
      },
      "source": [
        "### 2. Download and Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2b90HnObMkm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde4d369-06a2-4a3e-b586-409d2aa7ed1f"
      },
      "source": [
        "# download IMDB dataset\n",
        "!wget \"https://raw.githubusercontent.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/master/movie_data.csv\" -O \"movie_data.csv\"  \n",
        "\n",
        "# list files in current directory\n",
        "!ls -lah  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-15 17:46:12--  https://raw.githubusercontent.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/master/movie_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65862309 (63M) [text/plain]\n",
            "Saving to: ‘movie_data.csv’\n",
            "\n",
            "movie_data.csv      100%[===================>]  62.81M   163MB/s    in 0.4s    \n",
            "\n",
            "2022-04-15 17:46:17 (163 MB/s) - ‘movie_data.csv’ saved [65862309/65862309]\n",
            "\n",
            "total 63M\n",
            "drwxr-xr-x 1 root root 4.0K Apr 15 17:46 .\n",
            "drwxr-xr-x 1 root root 4.0K Apr 15 17:45 ..\n",
            "drwxr-xr-x 1 root root 4.0K Apr  8 13:31 .config\n",
            "-rw-r--r-- 1 root root  63M Apr 15 17:46 movie_data.csv\n",
            "drwxr-xr-x 1 root root 4.0K Apr  8 13:32 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v-9xJvhbb9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e1a248a0-930a-4d2a-dc23-490774ed0f40"
      },
      "source": [
        "# the path to the IMDB dataset\n",
        "dataset_path = 'movie_data.csv'  \n",
        "\n",
        "# read file (dataset) into our program using pandas\n",
        "data = pd.read_csv(dataset_path) \n",
        "\n",
        "# display first 5 rows\n",
        "data.head()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  I went and saw this movie last night after bei...          1\n",
              "1  Actor turned director Bill Paxton follows up h...          1\n",
              "2  As a recreational golfer with some knowledge o...          1\n",
              "3  I saw this film in a sneak preview, and it is ...          1\n",
              "4  Bill Paxton has taken the true story of the 19...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d213cbb-1a15-4ce2-a5d0-77c7fbc5832a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I went and saw this movie last night after bei...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As a recreational golfer with some knowledge o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d213cbb-1a15-4ce2-a5d0-77c7fbc5832a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4d213cbb-1a15-4ce2-a5d0-77c7fbc5832a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4d213cbb-1a15-4ce2-a5d0-77c7fbc5832a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sAgqwiZbzuU"
      },
      "source": [
        "### 3. Clean Text\n",
        "\n",
        "Define the `clean_review` function to apply on the dataset reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyrg00Ycb08M"
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "english_stopwords = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def clean_review(text):\n",
        "  # convert to lower case\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove none alphabetic characters\n",
        "  text = re.sub(r'[^a-z]', ' ', text)\n",
        "\n",
        "  # stem words \n",
        "  # split into words\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # stemming of words\n",
        "  stemmed = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "  text = ' '.join(stemmed)\n",
        "\n",
        "  # remove stopwords\n",
        "  text = ' '.join([word for word in text.split() if word not in english_stopwords])\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "# apply to all dataset\n",
        "data['clean_review'] = data['review'].apply(clean_review)\n",
        "data.head() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pSqLwbYcNqu"
      },
      "source": [
        "### 4. Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZlGI1CScPLu"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "X = data['clean_review'].values\n",
        "y = data['sentiment'].values\n",
        "\n",
        "# Split data into 50% training & 50% test\n",
        "# let's all use a random state of 42 for example to ensure having the same split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjYirO0ucXUw"
      },
      "source": [
        "### 5. Feature Extraction with Word Embeddings\n",
        "\n",
        "Instead of going with Bag-of-Words for feature extraction, we are using Keras'  `Tokenizer()` class to prepare the data for the `Embedding` layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuS3-0_3cfUP"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# define your tokenizer (with num_words=10000)\n",
        "tokenizer_obj = Tokenizer(num_words=10000)\n",
        "\n",
        "# assign an index (number) to each word using fit_on_texts function  \n",
        "tokenizer_obj.fit_on_texts(x_train)\n",
        "\n",
        "# will be used later to pad sequences\n",
        "max_length = 120\n",
        "\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "\n",
        "# transform each text to a sequence of integers (to be used later in embeddings layer)\n",
        "X_train_tokens =  tokenizer_obj.texts_to_sequences(x_train)\n",
        "X_test_tokens = tokenizer_obj.texts_to_sequences(x_test)\n",
        "\n",
        "# apply post-padding to the sequences\n",
        "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ67srOxdbu8"
      },
      "source": [
        "x_train[0], X_train_pad[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqg6oLbyeMzM"
      },
      "source": [
        "## Recurrent Neural Network\n",
        "\n",
        "Now it's time to build the deep RNN network that will model the data. The network has to start with an `Embedding` layer, then we add one or multiple Recurrent layers and finally finish with a couple of Dense layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8GdbHJ2T_QO"
      },
      "source": [
        "### Building and Training the RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqAkP-UodqOE"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "# FILL BLANKS\n",
        "# build the neural network\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "model.add(LSTM(75))\n",
        "model.add(Dense(50, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# compile model: assign loss & optimizer\n",
        "model.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlKvxbu8fpMK"
      },
      "source": [
        "# train model\n",
        "model.fit(X_train_pad, y_train, batch_size=32, epochs=5, validation_data=(X_test_pad, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkXHWbrCg0PD"
      },
      "source": [
        "### Plot training details\n",
        "\n",
        "We visualize the training parameters to have a better understanding of the model's convergence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqZrCxAGfxp_"
      },
      "source": [
        "def plot_accuracy_and_loss(model):\n",
        "    epochs = model.history.params['epochs']\n",
        "    epochs = range(epochs)\n",
        "    val_loss = model.history.history['val_loss']\n",
        "    val_accuracy = model.history.history['val_accuracy']\n",
        "    training_loss = model.history.history['loss']\n",
        "    training_accuracy = model.history.history['accuracy']\n",
        "\n",
        "    plt.plot(epochs, val_loss, 'r', label='test')\n",
        "    plt.plot(epochs, training_loss, 'b', label='training')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(epochs, val_accuracy, 'r', label='test')\n",
        "    plt.plot(epochs, training_accuracy, 'b', label='training')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_accuracy_and_loss(model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}